# Prepare prediction models
# Prepare input data format for LSTM and Attention models
   #!/usr/bin/perl -w
   use strict;

   =pod
   Usage: perl format.pl AMP.fa P > amp.txt
   Here, P means positive true lable, N means negative false lable, and
   none means do not need using this function, using for predicted 
   file re-format shuold silence this function
   =cut

   my $in = $ARGV[0];
   my %aacode =
       (
       A => "1", C => "2", D => "3", E => "4",
       F => "5", G => "6", H => "7", I => "8",
       K => "9", L => "10", M => "11", N => "12",
       P => "13", Q => "14", R => "15", S => "16",
       T => "17", V => "18", W => "19", Y => "20",
       );
   my ($a,$b);
   open I,"<$in";
   while(defined($a=<I>)){
       chomp($a);
       $a = uc $a;
       if($a =~ /\>/){
           $b = "0";
           }else{
           if($a =~ /B/){}
           elsif($a =~ /J/){}
           elsif($a =~ /O/){}
           elsif($a =~ /U/){}
           elsif($a =~ /X/){}
           elsif($a =~ /Z/){}
           else{
               my @seq = split //,$a;
               my $len = @seq;
               my $i = 0;
               while($i < $len){
                   $b = $b.",".$aacode{$seq[$i]};
                   $i = $i + 1;
               }
               my @bb = split /,/, $b;
               my $lb = @bb;
               my $cha = 300 - $lb;
               my $j = 0;
               while($j < $cha){
                   $b = "0".",".$b;
                   $j = $j + 1;
               }
               if($ARGV[1] eq "P" | $ARGV[1] eq "p"){
                   $b = $b.",1";
                   print "$b\n";
               }elsif($ARGV[1] eq "N" | $ARGV[1] eq "n"){
                   $b = $b.",0";
                   print "$b\n";
               }elsif($ARGV[1] eq "none"){
                   print "$b\n";
               }
           }
       }
   }
   close I;
# Attention Model
# usage python3 prediction_att.py amp.txt att_bact.txt
from keras.models import load_model
   from numpy import loadtxt, savetxt
   from Attention import Attention_layer
   from sys import argv

   model = load_model('att.h5', custom_objects={'Attention_layer': Attention_layer})
   x = loadtxt(argv[1], delimiter=",")

   preds = model.predict(x)
   savetxt(argv[2], preds, fmt="%.8f", delimiter=",")
# Attention_layer from Attention.py
   # -*- coding:utf-8 -*-
   from keras import backend as K
   from keras.engine.topology import Layer
   from keras import initializers, regularizers, constraints

   class Attention_layer(Layer):
       def __init__(self,
                    W_regularizer=None, b_regularizer=None,
                    W_constraint=None, b_constraint=None,
                    bias=True, **kwargs):
           self.supports_masking = True
           self.init = initializers.get('glorot_uniform')
           self.W_regularizer = regularizers.get(W_regularizer)
           self.b_regularizer = regularizers.get(b_regularizer)
           self.W_constraint = constraints.get(W_constraint)
           self.b_constraint = constraints.get(b_constraint)
           self.bias = bias
           super(Attention_layer, self).__init__(**kwargs)

       def build(self, input_shape):
           assert len(input_shape) == 3
           self.W = self.add_weight((input_shape[-1], input_shape[-1],),
                                    initializer=self.init,
                                    name='{}_W'.format(self.name),
                                    regularizer=self.W_regularizer,
                                    constraint=self.W_constraint)
           if self.bias:
               self.b = self.add_weight((input_shape[-1],),
                                        initializer='zero',
                                        name='{}_b'.format(self.name),
                                        regularizer=self.b_regularizer,
                                        constraint=self.b_constraint)

           super(Attention_layer, self).build(input_shape)

       def compute_mask(self, input, input_mask=None):
           return None

       def call(self, x, mask=None):
           uit = K.dot(x, self.W)

           if self.bias:
               uit += self.b

           uit = K.tanh(uit)
           a = K.exp(uit)
           if mask is not None:
               a *= K.cast(mask, K.floatx())
           a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())
           print(a)
           print(x)
           weighted_input = x * a
           print(weighted_input)
           return K.sum(weighted_input, axis=1)

       def compute_output_shape(self, input_shape):
           return (input_shape[0], input_shape[-1])
  # LSTM Model
  # usage python3 prediction_lstm.py AMP.txt lstm_bact.txt
  from keras.models import load_model
  from numpy import loadtxt, savetxt
  from sys import argv

  model = load_model('lstm.h5')
  x = loadtxt(argv[1], delimiter=",")

  preds = model.predict(x)
  savetxt(argv[2], preds, fmt="%.8f", delimiter=",")
  # Result integration
     #!/usr/bin/perl -w
   use strict;
   my $att = "att_bact.txt"; # result from attention model prediction
   my $lstm = "lstm_bact.txt"; # result from LSTM model prediction
  # my $XXXX = "XXXX_bact.txt"; # result from XXXX model prediction
   my $seq = "sequences_using_for_AMP_prediction.fa";
   my $a;
   my $l;
  # my $X;
   my $i = 1;
   my %h;
   open ATT, "<$att";
   while(defined($a=<ATT>)){
     $a =~ s/\s//igm;
     my $tmp;
     if($a > 0.5){
       $tmp = 1;
     }else{
       $tmp = 0;
     }
     $h{$i} = $tmp;
     $i++;
   }
   close ATT;

   $i = 1;
   open LSTM, "<$lstm";
   while(defined($l=<LSTM>)){
     $l =~ s/\s//igm;
     my $tmp;
     if($l > 0.5){
       $tmp = 1;
     }else{
       $tmp = 0;
     }
     $h{$i} = $h{$i} + $tmp;
     $i++;
   }
   close LSTM;

  # $i = 1;
  # open XXXX, "<$XXXX";
  # while(defined($b=<XXXX>)){
  #   $b =~ s/\s//igm;
  #   my $tmp;
  #  if($b > 0.5){
  #     $tmp = 1;
  #   }else{
  #     $tmp = 0;
  #   }
  #   $h{$i} = $h{$i} + $tmp;
  #   $i++;
  # }
  # close XXXX;

   print "There are $i sequences need to predictive.\n";
   print "name;size;AMP_prediction(1/0);\n";
   my $se;
   my %sequence;
   my $seq_name;
   my $j = 1;
   open SEQ, "<$seq";
   while(defined($se=<SEQ>)){
     $se =~ s/\s//igm;
     if($se =~ />/){
       my $pr;
       if($h{$j} == 2){
         $pr = 1;
       }else{
         $pr = 0;
       }
       $se = $se."|".$pr;
       $seq_name = $se;
       $j++;
     }else{
       $sequence{$seq_name} = $se;
     }
   }
   close SEQ;

   foreach my $k (keys %sequence){
       if($k =~ /\|1/){ # 1 means sequences with AMP activateï¼Œchange $k =~ /\|1/ to $k =~ /\|/ will output all sequences with prediction values.
           print "$k\n$sequence{$k}\n";
       }
   }
